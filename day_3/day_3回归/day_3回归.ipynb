{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7260a9cc-d526-4785-b7a9-f093b503957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 0, 训练集损失: 0.9101608267852238, 测试集损失: 0.6884080767631531\n",
      "训练次数: 1, 训练集损失: 0.7692265482175917, 测试集损失: 0.6205475926399231\n",
      "训练次数: 2, 训练集损失: 0.6517136707192376, 测试集损失: 0.5835627317428589\n",
      "训练次数: 3, 训练集损失: 0.5466629082248324, 测试集损失: 0.5429505705833435\n",
      "训练次数: 4, 训练集损失: 0.47889794196401325, 测试集损失: 0.5359954237937927\n",
      "训练次数: 5, 训练集损失: 0.4190204654421125, 测试集损失: 0.542162299156189\n",
      "训练次数: 6, 训练集损失: 0.38366842837560744, 测试集损失: 0.5662252306938171\n",
      "训练次数: 7, 训练集损失: 0.36919859974157243, 测试集损失: 0.5897507071495056\n",
      "训练次数: 8, 训练集损失: 0.347181926880564, 测试集损失: 0.5980970859527588\n",
      "训练次数: 9, 训练集损失: 0.34501995146274567, 测试集损失: 0.6149763464927673\n",
      "训练次数: 10, 训练集损失: 0.3317767151054882, 测试集损失: 0.6111993789672852\n",
      "训练次数: 11, 训练集损失: 0.32333513420252574, 测试集损失: 0.6160455942153931\n",
      "训练次数: 12, 训练集损失: 0.321102595045453, 测试集损失: 0.6543641686439514\n",
      "训练次数: 13, 训练集损失: 0.3129508133445467, 测试集损失: 0.6387500166893005\n",
      "训练次数: 14, 训练集损失: 0.3132401981524059, 测试集损失: 0.6211321353912354\n",
      "训练次数: 15, 训练集损失: 0.30806732177734375, 测试集损失: 0.6151152849197388\n",
      "训练次数: 16, 训练集损失: 0.30000259620802744, 测试集损失: 0.6517181992530823\n",
      "训练次数: 17, 训练集损失: 0.3023108882563455, 测试集损失: 0.6197920441627502\n",
      "训练次数: 18, 训练集损失: 0.2950173524164018, 测试集损失: 0.6258103847503662\n",
      "训练次数: 19, 训练集损失: 0.29213522161756245, 测试集损失: 0.6221840381622314\n",
      "训练次数: 20, 训练集损失: 0.29176468224752516, 测试集损失: 0.626045823097229\n",
      "训练次数: 21, 训练集损失: 0.2879729583149865, 测试集损失: 0.6066248416900635\n",
      "训练次数: 22, 训练集损失: 0.2964194927896772, 测试集损失: 0.603501558303833\n",
      "训练次数: 23, 训练集损失: 0.2860194444656372, 测试集损失: 0.6282222270965576\n",
      "训练次数: 24, 训练集损失: 0.27199492745456244, 测试集损失: 0.6154420375823975\n",
      "训练次数: 25, 训练集损失: 0.2700751438027337, 测试集损失: 0.6273441314697266\n",
      "训练次数: 26, 训练集损失: 0.27053631842136383, 测试集损失: 0.6034145951271057\n",
      "训练次数: 27, 训练集损失: 0.26962834951423464, 测试集损失: 0.5963958501815796\n",
      "训练次数: 28, 训练集损失: 0.2667256380830492, 测试集损失: 0.5866961479187012\n",
      "训练次数: 29, 训练集损失: 0.26280511844725835, 测试集损失: 0.6134862899780273\n",
      "训练次数: 30, 训练集损失: 0.27370838537102654, 测试集损失: 0.6121688485145569\n",
      "训练次数: 31, 训练集损失: 0.2617417730036236, 测试集损失: 0.5975528359413147\n",
      "训练次数: 32, 训练集损失: 0.2608085469830604, 测试集损失: 0.6042924523353577\n",
      "训练次数: 33, 训练集损失: 0.2560628631285259, 测试集损失: 0.5984129905700684\n",
      "训练次数: 34, 训练集损失: 0.250694541704087, 测试集损失: 0.5959194898605347\n",
      "训练次数: 35, 训练集损失: 0.25240133951107663, 测试集损失: 0.590997576713562\n",
      "训练次数: 36, 训练集损失: 0.24993003266198294, 测试集损失: 0.5854985117912292\n",
      "训练次数: 37, 训练集损失: 0.24507422567833037, 测试集损失: 0.5784956216812134\n",
      "训练次数: 38, 训练集损失: 0.24997782636256444, 测试集损失: 0.6080100536346436\n",
      "训练次数: 39, 训练集损失: 0.24370580060141428, 测试集损失: 0.5768845677375793\n",
      "训练次数: 40, 训练集损失: 0.24884208611079625, 测试集损失: 0.5803210139274597\n",
      "训练次数: 41, 训练集损失: 0.24021097698381969, 测试集损失: 0.5756503343582153\n",
      "训练次数: 42, 训练集损失: 0.23631655424833298, 测试集损失: 0.565181314945221\n",
      "训练次数: 43, 训练集损失: 0.2461768640648751, 测试集损失: 0.5505979061126709\n",
      "训练次数: 44, 训练集损失: 0.23762266692661105, 测试集损失: 0.552175760269165\n",
      "训练次数: 45, 训练集损失: 0.25470605350676034, 测试集损失: 0.5215057134628296\n",
      "训练次数: 46, 训练集损失: 0.23990832446586519, 测试集损失: 0.587540328502655\n",
      "训练次数: 47, 训练集损失: 0.2350097160254206, 测试集损失: 0.5734015107154846\n",
      "训练次数: 48, 训练集损失: 0.23263877346402123, 测试集损失: 0.5622960329055786\n",
      "训练次数: 49, 训练集损失: 0.2469172477722168, 测试集损失: 0.5502391457557678\n",
      "训练次数: 50, 训练集损失: 0.23245471715927124, 测试集损失: 0.5753955245018005\n",
      "训练次数: 51, 训练集损失: 0.23600721607605615, 测试集损失: 0.5787050724029541\n",
      "训练次数: 52, 训练集损失: 0.22977636044933683, 测试集损失: 0.5573198795318604\n",
      "训练次数: 53, 训练集损失: 0.22854614896433695, 测试集损失: 0.5298559665679932\n",
      "训练次数: 54, 训练集损失: 0.2280615834253175, 测试集损失: 0.5570029616355896\n",
      "训练次数: 55, 训练集损失: 0.22456102818250656, 测试集损失: 0.5217148661613464\n",
      "训练次数: 56, 训练集损失: 0.22812549698920476, 测试集损失: 0.5743791460990906\n",
      "训练次数: 57, 训练集损失: 0.22876258300883429, 测试集损失: 0.5231387615203857\n",
      "训练次数: 58, 训练集损失: 0.2339017064798446, 测试集损失: 0.5470059514045715\n",
      "训练次数: 59, 训练集损失: 0.2335381479490371, 测试集损失: 0.5874460935592651\n",
      "训练次数: 60, 训练集损失: 0.23029131122997828, 测试集损失: 0.5516843795776367\n",
      "训练次数: 61, 训练集损失: 0.2208226358606702, 测试集损失: 0.524099588394165\n",
      "训练次数: 62, 训练集损失: 0.22108880820728483, 测试集损失: 0.5619437098503113\n",
      "训练次数: 63, 训练集损失: 0.22165316343307495, 测试集损失: 0.5213785171508789\n",
      "训练次数: 64, 训练集损失: 0.21926839365845635, 测试集损失: 0.5480713844299316\n",
      "训练次数: 65, 训练集损失: 0.21538852013292767, 测试集损失: 0.5254970192909241\n",
      "训练次数: 66, 训练集损失: 0.21949217716852823, 测试集损失: 0.5277042388916016\n",
      "训练次数: 67, 训练集损失: 0.22311509613479888, 测试集损失: 0.5619348883628845\n",
      "训练次数: 68, 训练集损失: 0.2136915170011066, 测试集损失: 0.5125291347503662\n",
      "训练次数: 69, 训练集损失: 0.21470186291705995, 测试集损失: 0.5254796147346497\n",
      "训练次数: 70, 训练集损失: 0.21485713975770132, 测试集损失: 0.5217649340629578\n",
      "训练次数: 71, 训练集损失: 0.2129068509453819, 测试集损失: 0.5634152889251709\n",
      "训练次数: 72, 训练集损失: 0.21490508495342164, 测试集损失: 0.5110847353935242\n",
      "训练次数: 73, 训练集损失: 0.21544494089626132, 测试集损失: 0.5249658823013306\n",
      "训练次数: 74, 训练集损失: 0.21227020557437623, 测试集损失: 0.5394094586372375\n",
      "训练次数: 75, 训练集损失: 0.21301008122307913, 测试集损失: 0.5154817700386047\n",
      "训练次数: 76, 训练集损失: 0.20828841307333537, 测试集损失: 0.491780549287796\n",
      "训练次数: 77, 训练集损失: 0.21670240270239965, 测试集损失: 0.516464114189148\n",
      "训练次数: 78, 训练集损失: 0.21301012024992988, 测试集损失: 0.49168819189071655\n",
      "训练次数: 79, 训练集损失: 0.2155200264283589, 测试集损失: 0.49981486797332764\n",
      "训练次数: 80, 训练集损失: 0.21157406873646237, 测试集损失: 0.4933360815048218\n",
      "训练次数: 81, 训练集损失: 0.20893342509156182, 测试集损失: 0.5363315939903259\n",
      "训练次数: 82, 训练集损失: 0.20660665382941565, 测试集损失: 0.520416796207428\n",
      "训练次数: 83, 训练集损失: 0.21522234947908492, 测试集损失: 0.5220862627029419\n",
      "训练次数: 84, 训练集损失: 0.2075252476192656, 测试集损失: 0.5005770325660706\n",
      "训练次数: 85, 训练集损失: 0.20727018550747917, 测试集损失: 0.5171782374382019\n",
      "训练次数: 86, 训练集损失: 0.20701116429907934, 测试集损失: 0.49085038900375366\n",
      "训练次数: 87, 训练集损失: 0.20460223974216551, 测试集损失: 0.5229156613349915\n",
      "训练次数: 88, 训练集损失: 0.20565851813270933, 测试集损失: 0.5540018677711487\n",
      "训练次数: 89, 训练集损失: 0.20570097118616104, 测试集损失: 0.5610964298248291\n",
      "训练次数: 90, 训练集损失: 0.2599195156778608, 测试集损失: 0.4886893630027771\n",
      "训练次数: 91, 训练集损失: 0.22420948921214967, 测试集损失: 0.5330601930618286\n",
      "训练次数: 92, 训练集损失: 0.22445701488426753, 测试集损失: 0.5066202282905579\n",
      "训练次数: 93, 训练集损失: 0.21025428530715762, 测试集损失: 0.5408870577812195\n",
      "训练次数: 94, 训练集损失: 0.2008019604143642, 测试集损失: 0.5331082940101624\n",
      "训练次数: 95, 训练集损失: 0.2014023501958166, 测试集损失: 0.5130288600921631\n",
      "训练次数: 96, 训练集损失: 0.20192724821113406, 测试集损失: 0.48525476455688477\n",
      "训练次数: 97, 训练集损失: 0.20586548284405753, 测试集损失: 0.5024314522743225\n",
      "训练次数: 98, 训练集损失: 0.20686046956550508, 测试集损失: 0.4819292426109314\n",
      "训练次数: 99, 训练集损失: 0.20907820441893168, 测试集损失: 0.5005874037742615\n",
      "测试集预测值： [12048.861 17376.48  20029.11  19596.549 18768.7   12568.404 13258.953\n",
      " 10193.316 28381.639]\n",
      "模型平均误差： 4349.15869140625\n",
      "模型运行时间： 16.09966519999989\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import time\n",
    "strat = time.perf_counter()#高性能计时器，用于性能测试\n",
    "\n",
    "#读取训练数据和测试数据\n",
    "o_train = pd.read_csv('./train.csv')\n",
    "o_test = pd.read_csv('./test.csv')\n",
    "\n",
    "#自己的数据集，需要对原始数据进行处理\n",
    "#原数据 第一列是序号， 从第二列到导数第二列都是 维度，最后一列是房价\n",
    "#对各维度的预处理(标准化)方式：数值型的转为[-1,1]之间 z-score 标准化，新数据=（原数据-均值）/标准差\n",
    "#非数值型中的  无序型进行独热编码(one-hot encoding)，有序型 自己定义其数值 转换为数值型  本数据集默认全部为无序型\n",
    "#空值：每一个特征的全局平均值来代替无效值\n",
    "\n",
    "#将训练集与测试集的特征数据合并在一起 统一进行处理\n",
    "#loc：通过行标签索引数据 iloc：通过行号索引行数据 ix：通过行标签或行号索引数据（基于loc和iloc的混合）\n",
    "\n",
    "#合并数据集\n",
    "all_features = pd.concat((o_train.loc[:,'Area':'Neighborhood'],o_test.loc[:,'Area':'Neighborhood']))\n",
    "all_labels = pd.concat((o_train.loc[:,'Price'],o_test.loc[:,'Price']))\n",
    "#特征列从'Area'到'Neighborhood'，标签列是'Price'\n",
    "#训练集和测试机的数据进行合并\n",
    "#对特征值进行数据预处理\n",
    "#取出所有的数值型特征名称\n",
    "numeric_feats = all_features.dtypes[all_features.dtypes != \"object\"].index#数值类型\n",
    "object_feats = all_features.dtypes[all_features.dtypes == \"object\"].index#非数值类型\n",
    "\n",
    "# 将数值型特征进行 z-score 标准化\n",
    "all_features[numeric_feats] = all_features[numeric_feats].apply(lambda x: (x - x.mean()) / (x.std()))#标准化值 = (原始值 - 均值) / 标准差\n",
    "\n",
    "#对无序型进行one-hot encoding\n",
    "all_features = pd.get_dummies(all_features,prefix=object_feats, dummy_na=True)\n",
    "\n",
    "#空值：每一个特征的全局平均值来代替无效值 NA就是指空值\n",
    "all_features = all_features.fillna(all_features.mean())\n",
    "\n",
    "#对标签进行数据预处理\n",
    "#对标签进行 z-score 标准化\n",
    "mean = all_labels.mean()#计算均值\n",
    "std = all_labels.std()#计算标准差\n",
    "all_labels = (all_labels - mean)/std#标准化\n",
    "\n",
    "num_train = o_train.shape[0]\n",
    "train_features = all_features[:num_train].values.astype(np.float32)#(1314, 331)\n",
    "test_features = all_features[num_train:].values.astype(np.float32)#(146, 331)\n",
    "train_labels = all_labels[:num_train].values.astype(np.float32)\n",
    "test_labels = all_labels[num_train:].values.astype(np.float32)#将合并后的训练集和测试集进行分离\n",
    "#至此 输入数据准备完毕 可以看见 经过one-hot编码后  特征维度增加了很多 81->331\n",
    "\n",
    "#数据类型转换，数组转换成张量\n",
    "train_features = torch.from_numpy(train_features)\n",
    "train_labels = torch.from_numpy(train_labels).unsqueeze(1)\n",
    "test_features = torch.from_numpy(test_features)\n",
    "test_labels = torch.from_numpy(test_labels).unsqueeze(1)\n",
    "train_set = TensorDataset(train_features,train_labels)\n",
    "test_set = TensorDataset(test_features,test_labels)\n",
    "\n",
    "#设置迭代器\n",
    "train_data = DataLoader(dataset=train_set,batch_size=64,shuffle=True)\n",
    "test_data  = DataLoader(dataset=test_set,batch_size=64,shuffle=False)\n",
    "\n",
    "#设置网络结构\n",
    "class Net(torch.nn.Module):# 继承 torch 的 Module\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        # 定义每层用什么样的形式\n",
    "        self.layer1 = torch.nn.Linear(n_feature, 600)   #\n",
    "        self.layer2 = torch.nn.Linear(600, 1200)   #\n",
    "        self.layer3 = torch.nn.Linear(1200, n_output)\n",
    "\n",
    "    def forward(self, x):   # 这同时也是 Module 中的 forward 功能\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)      #\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)      #\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "net = Net(44,1)\n",
    "\n",
    "#反向传播算法 SGD Adam等\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "#均方损失函数\n",
    "criterion =\ttorch.nn.MSELoss()\n",
    "\n",
    "#记录用于绘图\n",
    "losses = []#记录每次迭代后训练的loss\n",
    "eval_losses = []#测试的\n",
    "\n",
    "for i in range(100):\n",
    "    train_loss = 0\n",
    "    # train_acc = 0\n",
    "    net.train() #网络设置为训练模式 暂时可加可不加\n",
    "    for tdata,tlabel in train_data:\n",
    "        #前向传播\n",
    "        y_ = net(tdata)\n",
    "        #记录单批次一次batch的loss\n",
    "        loss = criterion(y_, tlabel)\n",
    "        #反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #累计单批次误差\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "    losses.append(train_loss / len(train_data))\n",
    "    # 测试集进行测试 只用于进行评估\n",
    "    eval_loss = 0\n",
    "    net.eval()  # 可加可不加\n",
    "    for edata, elabel in test_data:\n",
    "        # 前向传播\n",
    "        y_ = net(edata)\n",
    "        # 记录单批次一次batch的loss，测试集就不需要反向传播更新网络了\n",
    "        loss = criterion(y_, elabel)\n",
    "        # 累计单批次误差\n",
    "        eval_loss = eval_loss + loss.item()\n",
    "    eval_losses.append(eval_loss / len(test_data))\n",
    "\n",
    "    print('训练次数: {}, 训练集损失: {}, 测试集损失: {}'.format(i, train_loss / len(train_data), eval_loss / len(test_data)))\n",
    "\n",
    "#测试最终模型的精准度，测试集的平均误差\n",
    "y_ = net(test_features)\n",
    "y_pre = y_ * std + mean#反求房价\n",
    "print('测试集预测值：',y_pre.squeeze().detach().cpu().numpy())\n",
    "print('模型平均误差：',abs(y_pre - (test_labels*std + mean)).mean().cpu().item() )\n",
    "end =time.perf_counter()\n",
    "print('模型运行时间：',end - strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128d691-3c93-4ce5-9e03-6b04dba791f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLR",
   "language": "python",
   "name": "dlr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
