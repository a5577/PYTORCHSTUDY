{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6543a033-e12c-4e62-9cfd-e39c566beb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动手练习：Seq2Seq实现机器翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269245d7-fa80-4acc-b159-befa7906ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter#计数统计（统计元素出现次数）\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk#import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb9cd32-321e-4826-975d-06b0bc82f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef5c87a-a4a3-4d78-80d1-60a87492b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集加载处理\n",
    "def load_data(in_file):\n",
    "    cn = []#存储中文句子\n",
    "    en = []#存储英文句子\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding='utf8') as f:## 以UTF-8编码打开文件，确保能正确处理中文字符\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            #文件格式：英文句子\\t中文句子\n",
    "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])#lower小写 word_tokenize分词\n",
    "            cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
    "    \n",
    "    return en, cn\n",
    "\n",
    "train_file = './train.txt'\n",
    "dev_file = './test.txt'\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f418e879-de31-4a7c-9876-5b6b7bda2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建单词表函数\n",
    "UNK_IDX = 0#处理为在词汇表中的词\n",
    "PAD_IDX = 1#用于将序列填充到相同长度\n",
    "\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_count[word] += 1\n",
    "    \n",
    "    ls = word_count.most_common(max_words)#获取出现频率最高的前max_words词\n",
    "    total_words = len(ls) + 2\n",
    "    \n",
    "    word_dict = {w[0] : index + 2 for index, w in enumerate(ls)}\n",
    "    word_dict['UNK'] = UNK_IDX\n",
    "    word_dict['PAD'] = PAD_IDX\n",
    "    \n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)#构建英文词典\n",
    "cn_dict, cn_total_words = build_dict(train_cn)#构建中文词典\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6ca259-92ab-4e98-a011-441562e5d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将单词全部转为数字\n",
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):#sort_by_len设置为True是为了使一个batch中的句子长度差不多\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "#按长度排序的辅助函数\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    #顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0881e29c-f769-4839-8c3c-832bfb998ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建批次索引 将几个句子作为一批\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches#输出为批次\n",
    "#数据填充对齐\n",
    "def prepare_data(seqs):   \n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    \n",
    "    return x, x_lengths\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)    \n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    \n",
    "    return all_ex   \n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8a7463-d022-489c-a729-22be70ac8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf11506c-ca38-478b-af44-540eae58d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置损失函数\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask#计算负对数似然\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae2660d-365c-4290-83f3-9246ccc54c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集: torch.Size([64, 8]) torch.Size([64])\n",
      "torch.Size([64, 8, 100]) torch.Size([1, 64, 100])\n",
      "torch.Size([64, 100]) \n",
      " tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)#词嵌入层\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)#GRU层/RNN\n",
    "        self.dropout = nn.Dropout(dropout)#dropout\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)#序列按长度降序排列\n",
    "        x_sorted = x[sorted_idx.long()]#返回排序后的长度和对应的索引\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hidden = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        \n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hidden = hidden[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        return out, hidden[[-1]]\n",
    "\n",
    "#测试维度\n",
    "p = PlainEncoder(en_total_words, 100)\n",
    "\n",
    "mb_x = torch.from_numpy(train_data[0][0]).long()\n",
    "mb_x_len = torch.from_numpy(train_data[0][1]).long()\n",
    "print(\"数据集:\", mb_x.shape, mb_x_len.shape)\n",
    "\n",
    "o, h = p(mb_x, mb_x_len)\n",
    "\n",
    "print(o.shape, h.shape)\n",
    "print(o[:, -1].shape, '\\n', o[:, -1] == h)\n",
    "\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        \n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        output = F.log_softmax(self.fc(output_seq), -1)\n",
    "        \n",
    "        return output, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aad015fd-4182-48ce-a691-58f9af622acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_cut, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y, y_lengths, hid)\n",
    "            \n",
    "        return output, None\n",
    "    \n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        encoder_cut, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y, y_lengths=torch.ones(batch_size).long().to(device), hid=hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1) \n",
    "            preds.append(y)\n",
    "\n",
    "        return torch.cat(preds, 1), None\n",
    "\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encode = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "model = PlainSeq2Seq(encode, decoder)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c80cdc1-c36c-46ff-978c-aa520292e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数:  0 迭代 0 loss: 1.4076074361801147\n",
      "迭代次数:  0 迭代 100 loss: 1.5525585412979126\n",
      "迭代次数:  0 迭代 200 loss: 2.25791072845459\n",
      "迭代次数:  0 迭代 300 loss: 1.7978249788284302\n",
      "迭代次数:  0 迭代 400 loss: 1.2585357427597046\n",
      "迭代次数:  0 迭代 500 loss: 1.8542312383651733\n",
      "迭代次数 0 训练损失 1.81715430173393\n",
      "损失评估 1.5388896465301514\n",
      "迭代次数:  1 迭代 0 loss: 1.3545321226119995\n",
      "迭代次数:  1 迭代 100 loss: 1.5669461488723755\n",
      "迭代次数:  1 迭代 200 loss: 2.2932708263397217\n",
      "迭代次数:  1 迭代 300 loss: 1.815649390220642\n",
      "迭代次数:  1 迭代 400 loss: 1.2158253192901611\n",
      "迭代次数:  1 迭代 500 loss: 1.8196275234222412\n",
      "迭代次数 1 训练损失 1.7961463607834547\n",
      "迭代次数:  2 迭代 0 loss: 1.360666275024414\n",
      "迭代次数:  2 迭代 100 loss: 1.5364938974380493\n",
      "迭代次数:  2 迭代 200 loss: 2.2342727184295654\n",
      "迭代次数:  2 迭代 300 loss: 1.790238857269287\n",
      "迭代次数:  2 迭代 400 loss: 1.224421501159668\n",
      "迭代次数:  2 迭代 500 loss: 1.7448128461837769\n",
      "迭代次数 2 训练损失 1.780696681307134\n",
      "迭代次数:  3 迭代 0 loss: 1.3125011920928955\n",
      "迭代次数:  3 迭代 100 loss: 1.5186328887939453\n",
      "迭代次数:  3 迭代 200 loss: 2.213425636291504\n",
      "迭代次数:  3 迭代 300 loss: 1.8208574056625366\n",
      "迭代次数:  3 迭代 400 loss: 1.2342723608016968\n",
      "迭代次数:  3 迭代 500 loss: 1.769189715385437\n",
      "迭代次数 3 训练损失 1.7621419579849769\n",
      "迭代次数:  4 迭代 0 loss: 1.3206632137298584\n",
      "迭代次数:  4 迭代 100 loss: 1.4682778120040894\n",
      "迭代次数:  4 迭代 200 loss: 2.2198822498321533\n",
      "迭代次数:  4 迭代 300 loss: 1.732488751411438\n",
      "迭代次数:  4 迭代 400 loss: 1.1931419372558594\n",
      "迭代次数:  4 迭代 500 loss: 1.7753616571426392\n",
      "迭代次数 4 训练损失 1.7457033474148116\n",
      "迭代次数:  5 迭代 0 loss: 1.281981348991394\n",
      "迭代次数:  5 迭代 100 loss: 1.427657961845398\n",
      "迭代次数:  5 迭代 200 loss: 2.2057955265045166\n",
      "迭代次数:  5 迭代 300 loss: 1.7825660705566406\n",
      "迭代次数:  5 迭代 400 loss: 1.2305257320404053\n",
      "迭代次数:  5 迭代 500 loss: 1.7293452024459839\n",
      "迭代次数 5 训练损失 1.7278640681526887\n",
      "损失评估 1.5486260652542114\n",
      "迭代次数:  6 迭代 0 loss: 1.2537803649902344\n",
      "迭代次数:  6 迭代 100 loss: 1.4960657358169556\n",
      "迭代次数:  6 迭代 200 loss: 2.1439168453216553\n",
      "迭代次数:  6 迭代 300 loss: 1.7063201665878296\n",
      "迭代次数:  6 迭代 400 loss: 1.1984548568725586\n",
      "迭代次数:  6 迭代 500 loss: 1.7594401836395264\n",
      "迭代次数 6 训练损失 1.7130780826339131\n",
      "迭代次数:  7 迭代 0 loss: 1.2481697797775269\n",
      "迭代次数:  7 迭代 100 loss: 1.4508349895477295\n",
      "迭代次数:  7 迭代 200 loss: 2.1767964363098145\n",
      "迭代次数:  7 迭代 300 loss: 1.631264567375183\n",
      "迭代次数:  7 迭代 400 loss: 1.1958729028701782\n",
      "迭代次数:  7 迭代 500 loss: 1.708799123764038\n",
      "迭代次数 7 训练损失 1.697793324977881\n",
      "迭代次数:  8 迭代 0 loss: 1.221003532409668\n",
      "迭代次数:  8 迭代 100 loss: 1.4017432928085327\n",
      "迭代次数:  8 迭代 200 loss: 2.1415188312530518\n",
      "迭代次数:  8 迭代 300 loss: 1.658755898475647\n",
      "迭代次数:  8 迭代 400 loss: 1.1250863075256348\n",
      "迭代次数:  8 迭代 500 loss: 1.7044321298599243\n",
      "迭代次数 8 训练损失 1.6810311172891452\n",
      "迭代次数:  9 迭代 0 loss: 1.2485202550888062\n",
      "迭代次数:  9 迭代 100 loss: 1.422090768814087\n",
      "迭代次数:  9 迭代 200 loss: 2.1862480640411377\n",
      "迭代次数:  9 迭代 300 loss: 1.650360107421875\n",
      "迭代次数:  9 迭代 400 loss: 1.1043219566345215\n",
      "迭代次数:  9 迭代 500 loss: 1.658564567565918\n",
      "迭代次数 9 训练损失 1.6655412787503396\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, num_epochs=1000):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            \n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            \n",
    "            mb_y_len = torch.from_numpy(mb_y_len - 1).to(device).long()\n",
    "            mb_y_len[mb_y_len <= 0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            #更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"迭代次数: \", epoch, '迭代', it, 'loss:', loss.item())\n",
    "                   \n",
    "        print(\"迭代次数\", epoch, \"训练损失\", total_loss / total_num_words)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "    print(\"损失评估\", total_loss / total_num_words)\n",
    "\n",
    "train(model, train_data, num_epochs=10)\n",
    "\n",
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    \n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f9013a-08f8-4000-84af-b21beeb2b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS classes begin tomorrow . EOS\n",
      "BOS 课 程 明 天 开 始 。 EOS\n",
      "明天上课。\n",
      "\n",
      "BOS choose what we like . EOS\n",
      "BOS 选 我 们 喜 欢 的 。 EOS\n",
      "服我们想知道。\n",
      "\n",
      "BOS choose one you like . EOS\n",
      "BOS 选 一 个 你 喜 欢 的 。 EOS\n",
      "选你喜欢的。\n",
      "\n",
      "BOS i want to eat candy . EOS\n",
      "BOS 我 想 吃 糖 。 EOS\n",
      "我想要一些甜的东西。\n",
      "\n",
      "BOS i was n't busy tomorrow . EOS\n",
      "BOS 我 明 天 不 忙 。 EOS\n",
      "我不明天是不懂。\n",
      "\n",
      "BOS do n't you speak chinese ? EOS\n",
      "BOS 你 会 说 中 文 吗 ? EOS\n",
      "你不会游泳吗？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入训练好模型\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))\n",
    "for i in range(0, 6):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3899b-3708-4206-b0e0-d0f0bdd7d64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
