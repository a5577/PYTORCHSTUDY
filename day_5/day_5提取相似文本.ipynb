{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861b1684-0766-43e2-a727-1c7de8833978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([256, 1])\n",
      "contexts_negatives shape: torch.Size([256, 60])\n",
      "masks shape: torch.Size([256, 60])\n",
      "labels shape: torch.Size([256, 60])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections#内置容器 例如数组等\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "with open('HarryPotter.txt', 'r') as f:#打开文件然后关闭\n",
    "    lines = f.readlines() # 该数据集中句子以换行符为分割\n",
    "    raw_dataset = [st.split() for st in lines] # st是sentence的缩写，单词以空格为分割\n",
    "\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st]) # tk是token的缩写  统计整个数据集中每个词出现的次数\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items())) # 只保留在数据集中至少出现5次的词  counter.items()：返回(词, 频次)的元组列表\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset] # raw_dataset中的单词在这一步被转换为对应的idx\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "\n",
    "#二次采样操作。越高频率的词一般意义不大，根据公式高频词越容易被过滤。准确来说，应该是降频操作。既不希望超高频被完全过滤，又希望减少高频词对训练的影响。\n",
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]#每一行句子进行一次 统计的也是每一行的token对应的idx数目\n",
    "\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:#遍历每个句子\n",
    "        if len(st) < 2:\n",
    "            continue#句子太短则跳过\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "#意思是用来得到每个单词前后的单词 用来展示它们之间的关系  中心词和周围词\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "\n",
    "#负采样近似加快程序运行时间\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    #all_contexts: 所有中心词的上下文列表\n",
    "    # 例如：[[45, 23], [12, 23, 67], ...]\n",
    "    # contexts[0] = 第一个中心词的上下文词索引列表\n",
    "    #sampling_weights: 每个词的采样权重列表\n",
    "    #长度 = 词汇表大小\n",
    "    #权重 ∝ 词频^0.75（论文建议）\n",
    "    #K: 每个上下文词对应的负样本数\n",
    "    #   通常K=5\n",
    "    all_negatives, neg_candidates, i = [], [], 0#存放所有负样本，预生成的候选负样本列表\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            \n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)#检查三个长度是否相等  也就是对应生成  中心词生成上下文 然后生成负样本\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "#返回单个样本的中心词 上下问和负样本\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    " #返回数据集大小   \n",
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]# 合并上下文和负样本，填充到max_len\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]#有效标记 其中负样本会被标记为1\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]#正确答案标记 其中只有上下文被标记为1\n",
    "        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "    return batch\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 0 if sys.platform.startswith('win32') else -1\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, #使用批处理函数\n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc1574a-4e69-4fcd-9048-038fac41b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 4.77, time 23.27s\n",
      "epoch 2, loss 1.78, time 23.85s\n",
      "epoch 3, loss 0.83, time 28.94s\n",
      "epoch 4, loss 0.48, time 27.75s\n",
      "epoch 5, loss 0.32, time 26.47s\n",
      "epoch 6, loss 0.24, time 25.35s\n",
      "epoch 7, loss 0.20, time 21.84s\n",
      "epoch 8, loss 0.18, time 28.71s\n",
      "epoch 9, loss 0.17, time 29.38s\n",
      "epoch 10, loss 0.17, time 24.25s\n",
      "epoch 11, loss 0.17, time 25.66s\n",
      "epoch 12, loss 0.17, time 24.50s\n",
      "epoch 13, loss 0.16, time 24.99s\n",
      "epoch 14, loss 0.17, time 21.73s\n",
      "epoch 15, loss 0.17, time 21.90s\n",
      "epoch 16, loss 0.17, time 21.81s\n",
      "epoch 17, loss 0.17, time 23.18s\n",
      "epoch 18, loss 0.17, time 21.62s\n",
      "epoch 19, loss 0.17, time 22.07s\n",
      "epoch 20, loss 0.17, time 21.75s\n",
      "epoch 21, loss 0.17, time 22.04s\n",
      "epoch 22, loss 0.17, time 21.81s\n",
      "epoch 23, loss 0.17, time 21.62s\n",
      "epoch 24, loss 0.17, time 21.42s\n",
      "epoch 25, loss 0.17, time 21.96s\n",
      "epoch 26, loss 0.17, time 21.14s\n",
      "epoch 27, loss 0.17, time 22.19s\n",
      "epoch 28, loss 0.17, time 22.37s\n",
      "epoch 29, loss 0.17, time 21.77s\n",
      "epoch 30, loss 0.17, time 22.17s\n",
      "epoch 31, loss 0.17, time 22.28s\n",
      "epoch 32, loss 0.17, time 21.85s\n",
      "epoch 33, loss 0.17, time 24.08s\n",
      "epoch 34, loss 0.17, time 21.37s\n",
      "epoch 35, loss 0.17, time 24.95s\n",
      "epoch 36, loss 0.17, time 25.62s\n",
      "epoch 37, loss 0.17, time 25.94s\n",
      "epoch 38, loss 0.16, time 26.39s\n",
      "epoch 39, loss 0.17, time 27.72s\n",
      "epoch 40, loss 0.17, time 21.95s\n",
      "epoch 41, loss 0.17, time 21.11s\n",
      "epoch 42, loss 0.16, time 26.83s\n",
      "epoch 43, loss 0.16, time 27.20s\n",
      "epoch 44, loss 0.16, time 27.10s\n",
      "epoch 45, loss 0.16, time 21.79s\n",
      "epoch 46, loss 0.16, time 21.90s\n",
      "epoch 47, loss 0.16, time 20.54s\n",
      "epoch 48, loss 0.16, time 20.57s\n",
      "epoch 49, loss 0.16, time 20.83s\n",
      "epoch 50, loss 0.16, time 20.60s\n"
     ]
    }
   ],
   "source": [
    "#采用交叉熵损失函数\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        # 1. 计算带权重的二分类交叉熵损失\n",
    "        res = res.sum(dim=1) / mask.float().sum(dim=1)\n",
    "        # 2. 按样本平均（考虑有效部分）\n",
    "        return res\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "embed_size = 200# 词向量维度\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),# 中心词嵌入\n",
    "                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)) # 目标词嵌入\n",
    "\n",
    "#skip_gram向前计算\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))# permute(0, 2, 1) = 交换维度1和维度2 bmm矩阵乘法\n",
    "    return pred\n",
    "\n",
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = loss(pred.view(label.shape), label, mask).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n",
    "\n",
    "train(net, 0.02, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceff7634-401b-4e62-b73c-92e7151da780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "余弦相似度 = 0.284: and\n",
      "余弦相似度 = 0.271: silver\n",
      "余弦相似度 = 0.258: small\n",
      "余弦相似度 = 0.256: History\n",
      "余弦相似度 = 0.246: Goyle\n"
     ]
    }
   ],
   "source": [
    "#测试模型\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('余弦相似度 = %.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('Jordan', 5, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d662d1-6b83-4fcb-927a-473b747dd8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
