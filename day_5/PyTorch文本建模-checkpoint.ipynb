{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f27d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动手练习：Word2vec提取相似文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa54fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([256, 1])\n",
      "contexts_negatives shape: torch.Size([256, 60])\n",
      "masks shape: torch.Size([256, 60])\n",
      "labels shape: torch.Size([256, 60])\n",
      "train on cpu\n",
      "epoch 1, loss 5.17, time 30.68s\n",
      "epoch 2, loss 3.15, time 31.78s\n",
      "epoch 3, loss 1.92, time 30.77s\n",
      "epoch 4, loss 1.21, time 29.52s\n",
      "epoch 5, loss 0.82, time 29.39s\n",
      "余弦相似度 = 0.256: turban.\n",
      "余弦相似度 = 0.245: thick,\n",
      "余弦相似度 = 0.238: crate\n",
      "余弦相似度 = 0.236: again,\n",
      "余弦相似度 = 0.223: station\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "with open('HarryPotter.txt', 'r') as f:\n",
    "    lines = f.readlines() # 该数据集中句子以换行符为分割\n",
    "    raw_dataset = [st.split() for st in lines] # st是sentence的缩写，单词以空格为分割\n",
    "\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st]) # tk是token的缩写\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items())) # 只保留在数据集中至少出现5次的词\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset] # raw_dataset中的单词在这一步被转换为对应的idx\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "\n",
    "#二次采样操作。越高频率的词一般意义不大，根据公式高频词越容易被过滤。准确来说，应该是降频操作。既不希望超高频被完全过滤，又希望减少高频词对训练的影响。\n",
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "\n",
    "#负采样近似加快程序运行时间\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            \n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "    return batch\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 0 if sys.platform.startswith('win32') else -1\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, \n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break\n",
    "\n",
    "#采用交叉熵损失函数\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        res = res.sum(dim=1) / mask.float().sum(dim=1)\n",
    "        return res\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "embed_size = 200\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))\n",
    "\n",
    "#skip_gram向前计算\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred\n",
    "\n",
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = loss(pred.view(label.shape), label, mask).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n",
    "\n",
    "train(net, 0.01, 5)\n",
    "\n",
    "#测试模型\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('余弦相似度 = %.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('Dursley', 5, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81ea1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff3e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动手练习：Seq2Seq实现机器翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f939e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集: torch.Size([64, 9]) torch.Size([64])\n",
      "torch.Size([64, 9, 100]) torch.Size([1, 64, 100])\n",
      "torch.Size([64, 100]) \n",
      " tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "迭代次数:  0 迭代 0 loss: 7.9240288734436035\n",
      "迭代次数:  0 迭代 100 loss: 4.954286098480225\n",
      "迭代次数:  0 迭代 200 loss: 4.346655368804932\n",
      "迭代次数:  0 迭代 300 loss: 4.635125637054443\n",
      "迭代次数:  0 迭代 400 loss: 4.609460830688477\n",
      "迭代次数:  0 迭代 500 loss: 4.263648986816406\n",
      "迭代次数 0 训练损失 4.731006864201709\n",
      "损失评估 3.1907880306243896\n",
      "迭代次数:  1 迭代 0 loss: 3.8907952308654785\n",
      "迭代次数:  1 迭代 100 loss: 3.7558422088623047\n",
      "迭代次数:  1 迭代 200 loss: 3.2824087142944336\n",
      "迭代次数:  1 迭代 300 loss: 3.850149393081665\n",
      "迭代次数:  1 迭代 400 loss: 3.8532021045684814\n",
      "迭代次数:  1 迭代 500 loss: 3.7996721267700195\n",
      "迭代次数 1 训练损失 3.7517296742820623\n",
      "迭代次数:  2 迭代 0 loss: 3.390547752380371\n",
      "迭代次数:  2 迭代 100 loss: 3.3264646530151367\n",
      "迭代次数:  2 迭代 200 loss: 2.887437582015991\n",
      "迭代次数:  2 迭代 300 loss: 3.470872402191162\n",
      "迭代次数:  2 迭代 400 loss: 3.4659101963043213\n",
      "迭代次数:  2 迭代 500 loss: 3.5183217525482178\n",
      "迭代次数 2 训练损失 3.39195244563888\n",
      "迭代次数:  3 迭代 0 loss: 3.1209330558776855\n",
      "迭代次数:  3 迭代 100 loss: 3.0677013397216797\n",
      "迭代次数:  3 迭代 200 loss: 2.6105456352233887\n",
      "迭代次数:  3 迭代 300 loss: 3.2026400566101074\n",
      "迭代次数:  3 迭代 400 loss: 3.2413296699523926\n",
      "迭代次数:  3 迭代 500 loss: 3.3570098876953125\n",
      "迭代次数 3 训练损失 3.161030057813074\n",
      "迭代次数:  4 迭代 0 loss: 2.8970725536346436\n",
      "迭代次数:  4 迭代 100 loss: 2.8832476139068604\n",
      "迭代次数:  4 迭代 200 loss: 2.4077370166778564\n",
      "迭代次数:  4 迭代 300 loss: 3.029244899749756\n",
      "迭代次数:  4 迭代 400 loss: 3.0532519817352295\n",
      "迭代次数:  4 迭代 500 loss: 3.1638967990875244\n",
      "迭代次数 4 训练损失 2.99155631043555\n",
      "迭代次数:  5 迭代 0 loss: 2.757211208343506\n",
      "迭代次数:  5 迭代 100 loss: 2.7020180225372314\n",
      "迭代次数:  5 迭代 200 loss: 2.283510446548462\n",
      "迭代次数:  5 迭代 300 loss: 2.8912391662597656\n",
      "迭代次数:  5 迭代 400 loss: 2.9255590438842773\n",
      "迭代次数:  5 迭代 500 loss: 3.066016912460327\n",
      "迭代次数 5 训练损失 2.8607203799838867\n",
      "损失评估 2.439427137374878\n",
      "迭代次数:  6 迭代 0 loss: 2.6259942054748535\n",
      "迭代次数:  6 迭代 100 loss: 2.592780113220215\n",
      "迭代次数:  6 迭代 200 loss: 2.1715760231018066\n",
      "迭代次数:  6 迭代 300 loss: 2.7945263385772705\n",
      "迭代次数:  6 迭代 400 loss: 2.8226828575134277\n",
      "迭代次数:  6 迭代 500 loss: 2.956697463989258\n",
      "迭代次数 6 训练损失 2.7537425091198333\n",
      "迭代次数:  7 迭代 0 loss: 2.5399131774902344\n",
      "迭代次数:  7 迭代 100 loss: 2.4802112579345703\n",
      "迭代次数:  7 迭代 200 loss: 2.0914037227630615\n",
      "迭代次数:  7 迭代 300 loss: 2.7304677963256836\n",
      "迭代次数:  7 迭代 400 loss: 2.6862761974334717\n",
      "迭代次数:  7 迭代 500 loss: 2.8875439167022705\n",
      "迭代次数 7 训练损失 2.667363010765063\n",
      "迭代次数:  8 迭代 0 loss: 2.4482810497283936\n",
      "迭代次数:  8 迭代 100 loss: 2.413273334503174\n",
      "迭代次数:  8 迭代 200 loss: 1.9834727048873901\n",
      "迭代次数:  8 迭代 300 loss: 2.6323742866516113\n",
      "迭代次数:  8 迭代 400 loss: 2.643190860748291\n",
      "迭代次数:  8 迭代 500 loss: 2.7920496463775635\n",
      "迭代次数 8 训练损失 2.58841740770988\n",
      "迭代次数:  9 迭代 0 loss: 2.336355686187744\n",
      "迭代次数:  9 迭代 100 loss: 2.314774751663208\n",
      "迭代次数:  9 迭代 200 loss: 1.9567680358886719\n",
      "迭代次数:  9 迭代 300 loss: 2.6123976707458496\n",
      "迭代次数:  9 迭代 400 loss: 2.5569565296173096\n",
      "迭代次数:  9 迭代 500 loss: 2.716510772705078\n",
      "迭代次数 9 训练损失 2.520652558456735\n",
      "BOS choose what we like . EOS\n",
      "BOS 选 我 们 喜 欢 的 。 EOS\n",
      "我们想要一些问题。\n",
      "\n",
      "BOS choose one you like . EOS\n",
      "BOS 选 一 个 你 喜 欢 的 。 EOS\n",
      "选你喜得好。\n",
      "\n",
      "BOS i want to eat candy . EOS\n",
      "BOS 我 想 吃 糖 。 EOS\n",
      "我想要一些牛奶。\n",
      "\n",
      "BOS i was n't busy tomorrow . EOS\n",
      "BOS 我 明 天 不 忙 。 EOS\n",
      "我不喜欢他的。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            \n",
    "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
    "            cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
    "    \n",
    "    return en, cn\n",
    "\n",
    "train_file = './data/train.txt'\n",
    "dev_file = './data/test.txt'\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)\n",
    "\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_count[word] += 1\n",
    "    \n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    \n",
    "    word_dict = {w[0] : index + 2 for index, w in enumerate(ls)}\n",
    "    word_dict['UNK'] = UNK_IDX\n",
    "    word_dict['PAD'] = PAD_IDX\n",
    "    \n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}\n",
    "\n",
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    #顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)\n",
    "\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):   \n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    \n",
    "    return x, x_lengths\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)    \n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    \n",
    "    return all_ex   \n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)\n",
    "\n",
    "#设置损失函数\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hidden = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        \n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hidden = hidden[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        return out, hidden[[-1]]\n",
    "\n",
    "#测试维度\n",
    "p = PlainEncoder(en_total_words, 100)\n",
    "\n",
    "mb_x = torch.from_numpy(train_data[0][0]).long()\n",
    "mb_x_len = torch.from_numpy(train_data[0][1]).long()\n",
    "print(\"数据集:\", mb_x.shape, mb_x_len.shape)\n",
    "\n",
    "o, h = p(mb_x, mb_x_len)\n",
    "\n",
    "print(o.shape, h.shape)\n",
    "print(o[:, -1].shape, '\\n', o[:, -1] == h)\n",
    "\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        \n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        output = F.log_softmax(self.fc(output_seq), -1)\n",
    "        \n",
    "        return output, hid\n",
    "        \n",
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_cut, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y, y_lengths, hid)\n",
    "            \n",
    "        return output, None\n",
    "    \n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        encoder_cut, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y, y_lengths=torch.ones(batch_size).long().to(device), hid=hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1) \n",
    "            preds.append(y)\n",
    "\n",
    "        return torch.cat(preds, 1), None\n",
    "\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encode = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "model = PlainSeq2Seq(encode, decoder)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            \n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            \n",
    "            mb_y_len = torch.from_numpy(mb_y_len - 1).to(device).long()\n",
    "            mb_y_len[mb_y_len <= 0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            #更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"迭代次数: \", epoch, '迭代', it, 'loss:', loss.item())\n",
    "                   \n",
    "        print(\"迭代次数\", epoch, \"训练损失\", total_loss / total_num_words)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "    print(\"损失评估\", total_loss / total_num_words)\n",
    "\n",
    "train(model, train_data, num_epochs=10)\n",
    "\n",
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    \n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "#导入训练好模型\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))\n",
    "for i in range(1, 5):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed89bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动手练习：Attention模型实现文本自动分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d9854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train data: 1000\n",
      "len of dev data: 200\n",
      "len of test data: 300\n",
      "3287\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_DIM = 100\n",
    "torch.manual_seed(99)\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(tokenize=lambda x: x.split(), lower=True)\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float)\n",
    "\n",
    "def get_dataset(corpur_path, text_field, label_field):\n",
    "    fields = [('text', text_field), ('label', label_field)]\n",
    "    examples = []\n",
    "    with open(corpur_path) as f:\n",
    "        li = []\n",
    "        while True:\n",
    "            content = f.readline().replace('\\n', '')\n",
    "            if not content:\n",
    "                if not li:\n",
    "                    break\n",
    "                label = li[0][10]\n",
    "                text = li[1][6:-7]\n",
    "                examples.append(torchtext.legacy.data.Example.fromlist([text, label], fields))\n",
    "                li = []\n",
    "            else:\n",
    "                li.append(content)\n",
    "\n",
    "    return examples, fields\n",
    "\n",
    "train_examples, train_fields = get_dataset(\"corpurs/trains.txt\", TEXT, LABEL)\n",
    "dev_examples, dev_fields = get_dataset(\"corpurs/dev.txt\", TEXT, LABEL)\n",
    "test_examples, test_fields = get_dataset(\"corpurs/tests.txt\", TEXT, LABEL)\n",
    "\n",
    "#构建数据集\n",
    "train_data = torchtext.legacy.data.Dataset(train_examples, train_fields)\n",
    "dev_data = torchtext.legacy.data.Dataset(dev_examples, dev_fields)\n",
    "test_data = torchtext.legacy.data.Dataset(test_examples, test_fields)\n",
    "\n",
    "print('len of train data:', len(train_data))\n",
    "print('len of dev data:', len(dev_data))\n",
    "print('len of test data:', len(test_data))\n",
    "\n",
    "#创建词向量\n",
    "TEXT.build_vocab(train_data, max_size=5000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_data)\n",
    "print(len(TEXT.vocab))\n",
    "\n",
    "#创建迭代器\n",
    "train_iterator, dev_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, dev_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9905b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=0.5)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "        self.w_omega = torch.nn.Parameter(torch.Tensor(hidden_dim * 2, hidden_dim * 2))\n",
    "        self.u_omega = torch.nn.Parameter(torch.Tensor(hidden_dim * 2, 1))\n",
    "        torch.nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "        torch.nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "\n",
    "    def attention_net(self, x):\n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega))\n",
    "        att = torch.matmul(u, self.u_omega)\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = x * att_score\n",
    "        context = torch.sum(scored_x, dim=1)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        output, (final_hidden_state, final_cell_state) = self.rnn(embedding)\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        attn_output = self.attention_net(output)\n",
    "        logit = self.fc(attn_output)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e543bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_embedding: torch.Size([3287, 100])\n",
      "embedding layer inited.\n"
     ]
    }
   ],
   "source": [
    "rnn = BiLSTM_Attention(len(TEXT.vocab), EMBEDDING_DIM, hidden_dim=64, n_layers=2)\n",
    "\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print('pretrained_embedding:', pretrained_embedding.shape)\n",
    "rnn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "print('embedding layer inited.')\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "criteon = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567ba35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数: 01 | 迭代时间: 0.0m 2.14s\n",
      "\t训练集损失: 0.694 | 训练集准确率: 49.21%\n",
      "\t验证集损失: 0.691 | 验证集准确率: 53.73%\n",
      "迭代次数: 02 | 迭代时间: 0.0m 2.22s\n",
      "\t训练集损失: 0.690 | 训练集准确率: 53.41%\n",
      "\t验证集损失: 0.683 | 验证集准确率: 57.29%\n",
      "迭代次数: 03 | 迭代时间: 0.0m 2.28s\n",
      "\t训练集损失: 0.686 | 训练集准确率: 55.20%\n",
      "\t验证集损失: 0.671 | 验证集准确率: 60.33%\n",
      "迭代次数: 04 | 迭代时间: 0.0m 2.24s\n",
      "\t训练集损失: 0.669 | 训练集准确率: 61.37%\n",
      "\t验证集损失: 0.641 | 验证集准确率: 61.72%\n",
      "迭代次数: 05 | 迭代时间: 0.0m 2.21s\n",
      "\t训练集损失: 0.638 | 训练集准确率: 63.38%\n",
      "\t验证集损失: 0.620 | 验证集准确率: 66.75%\n",
      "迭代次数: 06 | 迭代时间: 0.0m 2.15s\n",
      "\t训练集损失: 0.601 | 训练集准确率: 66.89%\n",
      "\t验证集损失: 0.610 | 验证集准确率: 70.62%\n",
      "迭代次数: 07 | 迭代时间: 0.0m 2.37s\n",
      "\t训练集损失: 0.546 | 训练集准确率: 72.25%\n",
      "\t验证集损失: 0.611 | 验证集准确率: 68.36%\n",
      "迭代次数: 08 | 迭代时间: 0.0m 2.13s\n",
      "\t训练集损失: 0.517 | 训练集准确率: 73.53%\n",
      "\t验证集损失: 0.592 | 验证集准确率: 72.57%\n",
      "迭代次数: 09 | 迭代时间: 0.0m 2.25s\n",
      "\t训练集损失: 0.484 | 训练集准确率: 76.01%\n",
      "\t验证集损失: 0.584 | 验证集准确率: 73.74%\n",
      "迭代次数: 10 | 迭代时间: 0.0m 2.23s\n",
      "\t训练集损失: 0.428 | 训练集准确率: 80.66%\n",
      "\t验证集损失: 0.639 | 验证集准确率: 71.57%\n",
      "迭代次数: 11 | 迭代时间: 0.0m 2.17s\n",
      "\t训练集损失: 0.412 | 训练集准确率: 80.26%\n",
      "\t验证集损失: 0.571 | 验证集准确率: 73.35%\n",
      "迭代次数: 12 | 迭代时间: 0.0m 2.17s\n",
      "\t训练集损失: 0.361 | 训练集准确率: 85.25%\n",
      "\t验证集损失: 0.622 | 验证集准确率: 74.13%\n",
      "迭代次数: 13 | 迭代时间: 0.0m 2.24s\n",
      "\t训练集损失: 0.360 | 训练集准确率: 84.62%\n",
      "\t验证集损失: 0.534 | 验证集准确率: 74.22%\n",
      "迭代次数: 14 | 迭代时间: 0.0m 2.31s\n",
      "\t训练集损失: 0.324 | 训练集准确率: 85.78%\n",
      "\t验证集损失: 0.536 | 验证集准确率: 76.61%\n",
      "迭代次数: 15 | 迭代时间: 0.0m 2.17s\n",
      "\t训练集损失: 0.300 | 训练集准确率: 86.65%\n",
      "\t验证集损失: 0.568 | 验证集准确率: 75.91%\n",
      "迭代次数: 16 | 迭代时间: 0.0m 2.22s\n",
      "\t训练集损失: 0.251 | 训练集准确率: 89.40%\n",
      "\t验证集损失: 0.552 | 验证集准确率: 77.39%\n",
      "迭代次数: 17 | 迭代时间: 0.0m 2.16s\n",
      "\t训练集损失: 0.223 | 训练集准确率: 91.35%\n",
      "\t验证集损失: 0.612 | 验证集准确率: 77.60%\n",
      "迭代次数: 18 | 迭代时间: 0.0m 2.19s\n",
      "\t训练集损失: 0.221 | 训练集准确率: 90.38%\n",
      "\t验证集损失: 0.622 | 验证集准确率: 76.61%\n",
      "迭代次数: 19 | 迭代时间: 0.0m 2.26s\n",
      "\t训练集损失: 0.185 | 训练集准确率: 92.98%\n",
      "\t验证集损失: 0.854 | 验证集准确率: 70.88%\n",
      "迭代次数: 20 | 迭代时间: 0.0m 2.17s\n",
      "\t训练集损失: 0.164 | 训练集准确率: 93.45%\n",
      "\t验证集损失: 0.680 | 验证集准确率: 77.39%\n",
      "迭代次数: 21 | 迭代时间: 0.0m 2.20s\n",
      "\t训练集损失: 0.126 | 训练集准确率: 95.52%\n",
      "\t验证集损失: 0.833 | 验证集准确率: 77.00%\n",
      "迭代次数: 22 | 迭代时间: 0.0m 2.20s\n",
      "\t训练集损失: 0.126 | 训练集准确率: 94.43%\n",
      "\t验证集损失: 0.770 | 验证集准确率: 76.78%\n",
      "迭代次数: 23 | 迭代时间: 0.0m 2.19s\n",
      "\t训练集损失: 0.117 | 训练集准确率: 95.49%\n",
      "\t验证集损失: 0.749 | 验证集准确率: 76.00%\n",
      "迭代次数: 24 | 迭代时间: 0.0m 2.25s\n",
      "\t训练集损失: 0.102 | 训练集准确率: 96.20%\n",
      "\t验证集损失: 0.894 | 验证集准确率: 70.49%\n",
      "迭代次数: 25 | 迭代时间: 0.0m 2.22s\n",
      "\t训练集损失: 0.083 | 训练集准确率: 97.15%\n",
      "\t验证集损失: 0.828 | 验证集准确率: 77.00%\n",
      "迭代次数: 26 | 迭代时间: 0.0m 2.21s\n",
      "\t训练集损失: 0.079 | 训练集准确率: 96.73%\n",
      "\t验证集损失: 0.658 | 验证集准确率: 76.61%\n",
      "迭代次数: 27 | 迭代时间: 0.0m 2.18s\n",
      "\t训练集损失: 0.069 | 训练集准确率: 97.42%\n",
      "\t验证集损失: 0.832 | 验证集准确率: 73.44%\n",
      "迭代次数: 28 | 迭代时间: 0.0m 2.24s\n",
      "\t训练集损失: 0.064 | 训练集准确率: 98.00%\n",
      "\t验证集损失: 0.836 | 验证集准确率: 75.69%\n",
      "迭代次数: 29 | 迭代时间: 0.0m 2.11s\n",
      "\t训练集损失: 0.052 | 训练集准确率: 98.00%\n",
      "\t验证集损失: 0.923 | 验证集准确率: 75.52%\n",
      "迭代次数: 30 | 迭代时间: 0.0m 2.18s\n",
      "\t训练集损失: 0.044 | 训练集准确率: 98.49%\n",
      "\t验证集损失: 1.257 | 验证集准确率: 69.70%\n",
      "测试集损失: 0.483 |  测试集准确率: 79.36%\n"
     ]
    }
   ],
   "source": [
    "#计算准确率\n",
    "def binary_acc(preds, y):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "#训练模型\n",
    "def train(rnn, iterator, optimizer, criteon):\n",
    "    avg_loss = []\n",
    "    avg_acc = []\n",
    "    rnn.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        pred = rnn(batch.text).squeeze()\n",
    "        loss = criteon(pred, batch.label)\n",
    "        acc = binary_acc(pred, batch.label).item()\n",
    "        avg_loss.append(loss.item())\n",
    "        avg_acc.append(acc)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "#评估函数\n",
    "def evaluate(rnn, iterator, criteon):\n",
    "    avg_loss = []\n",
    "    avg_acc = []\n",
    "    rnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = rnn(batch.text).squeeze()\n",
    "            loss = criteon(pred, batch.label)\n",
    "            acc = binary_acc(pred, batch.label).item()\n",
    "            avg_loss.append(loss.item())\n",
    "            avg_acc.append(acc)\n",
    "\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "#训练模型，并打印模型的表现\n",
    "best_valid_acc = float('-inf')\n",
    "\n",
    "for epoch in range(30):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(rnn, train_iterator, optimizer, criteon)\n",
    "    dev_loss, dev_acc = evaluate(rnn, dev_iterator, criteon)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    if dev_acc > best_valid_acc:\n",
    "        best_valid_acc = dev_acc\n",
    "        torch.save(rnn.state_dict(), 'wordavg-model.pt')\n",
    "\n",
    "    print(f'迭代次数: {epoch+1:02} | 迭代时间: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "    print(f'\\t训练集损失: {train_loss:.3f} | 训练集准确率: {train_acc*100:.2f}%')\n",
    "    print(f'\\t验证集损失: {dev_loss:.3f} | 验证集准确率: {dev_acc*100:.2f}%')\n",
    "\n",
    "#用保存的模型参数预测数据\n",
    "rnn.load_state_dict(torch.load(\"wordavg-model.pt\"))\n",
    "test_loss, test_acc = evaluate(rnn, test_iterator, criteon)\n",
    "print(f'测试集损失: {test_loss:.3f} |  测试集准确率: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4e508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
